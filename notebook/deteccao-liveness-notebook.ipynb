{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBA FIAP Inteligência Artificial & Machine Learning\n",
    "\n",
    "## Visão Computacional: Análise de Imagens Médicas\n",
    "\n",
    "> Atenção: este notebook foi desenhado para funcionar no **Google Collab**.\n",
    "\n",
    "\n",
    "## 1. Introdução\n",
    "\n",
    "Uma determinada fintech focada em consumidores finais pessoa física constataou um grande número de fraudes em transações bancárias.\n",
    "\n",
    "O setor de fraudes apontou que existem clientes que se queixaram de não contratar serviços específicos, como o crédito pessoal, e após isso transferir para outras contas desconhecidas. \n",
    "\n",
    "Após análises pelas equipes de segurança, os protocolos de utilização da senha foram realizados em conformidade, ou seja, cada cliente autenticou com sua própria senha de maneira regular.\n",
    "\n",
    "Em função disso, o banco precisa arcar com reembolsos e medidas de contenção para evitar processos judiciais, pois os clientes alegam terem sido invadidos por hackers ou algo parecido.\n",
    "\n",
    "Uma das formas de solucionar ou minimizar este problema é com a utilização de outras formas de autenticação, sobretudo em operações críticas, como a obtenção de crédito pessoal.\n",
    "\n",
    "Desta forma podemos implementar uma verificação de identidade com prova de vida (liveness), que utilize uma verificação e identificação facial. \n",
    "\n",
    "Caso o cliente não seja autenticado, ele será atendido por uma esteira dedicada e as evidências da não identificação serão encaminhadas para a área de IA para validação dos parâmetros e limiares para aperfeiçoamento do modelo.\n",
    "\n",
    "Será necessário construir:\n",
    "\n",
    "* Detector de faces\n",
    "* Identificação de faces (podendo ser um comparador entre um rosto de documento e outra da prova de vida)\n",
    "* Detecção de vivacidade (liveness) para evitar que um fraudador utilize uma foto estática.\n",
    "\n",
    "\n",
    ">Formas alternativas de prover a identificação e prova de vivacidade, além destas que foram solicitadas poderão ser submetidas.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"imagens/liveness.jpg\">\n",
    "</p>\n",
    "\n",
    "Imagem retirada do [Grunge](https://www.grunge.com/192826/company-testing-robocop-facial-recognition-software-with-us-police/).\n",
    "\n",
    "## 2. Instruções\n",
    "\n",
    "Este projeto final tem como objetivo explorar os conhecimentos adquiridos nas aulas práticas.\n",
    "\n",
    "Iremos constuir uma forma de validar se uma determinada imagem foi ou não adulterada e se trata de uma produção fraudade.\n",
    "\n",
    "Existem diversas formas de validar a vivacidade, e neste sentido conto com a criatividade de vocês dado que já dominam encontrar uma face numa imagem, aplicar marcos faciais e até mesmo construir uma rede neural convulacional.\n",
    "\n",
    "A abordagem mais simples é pela construção de uma rede neural com imagens de fotos de rostos de outras fotos e fotos de rostos sem modificações. Tal classificador deverá classificar se dada imagem possui vivacidade ou não com uma pontuação de probabilidade.\n",
    "\n",
    "Referências que abordam o tema para servir de inspiração:\n",
    "\n",
    "1. [PyImageSearch](https://pyimagesearch.com/2019/03/11/liveness-detection-with-opencv/), Liveness detection with OpenCV;\n",
    "2. [Kickertech](https://kickertech.com/face-liveness-detection-via-opencv-and-tensorflow/), Liveness detection via OpenCV and Tensorflow.\n",
    "3. [Towards Data Science](https://towardsdatascience.com/real-time-face-liveness-detection-with-python-keras-and-opencv-c35dc70dafd3?gi=24f8e1b740f9), Real-time face liveness detection with Python, Keras and OpenCV.\n",
    "\n",
    "Este projeto poderá ser feita por grupos de até 4 pessoas.\n",
    "Caso este projeto seja substitutivo, deverá ser realizado por apenas uma pessoa.\n",
    "\n",
    "| Nome dos Integrantes        | RM            | Turma |\n",
    "| :---------------------------| :------------ | :---: |\n",
    "| Izabela Ramos Ferreira      | RM 352447     | 5DTSR |\n",
    "| Kaique Vinicius Lima Soares | RM 351437     | 5DTSR |\n",
    "| Walder Octacilio Garbellott | RM 352469     | 5DTSR |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Abordagem e organização da solução do problema (2 pontos)\n",
    "\n",
    "Como o grupo pretende deteccar a prova de vivacidade de uma determinada imagem? Quais os passos e os building blocks deste processo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resposta**: \n",
    "\n",
    "\n",
    "Para detectar a prova de vivacidade (liveness) de uma determinada imagem, utilizaremos uma abordagem baseada em aprendizado de máquina, especificamente com uma rede neural convolucional (CNN) para classificar as imagens como reais ou fraudulentas. A detecção de vivacidade é essencial para garantir que a imagem apresentada seja de uma pessoa viva e não uma tentativa de fraude utilizando fotos, vídeos ou máscaras.\n",
    "\n",
    "### Passos e Building Blocks do Processo\n",
    "\n",
    "#### 1. Coleta e Preparação dos Dados\n",
    "- **Coleta de Dados:** Reunir um conjunto diversificado de imagens contendo rostos reais (de pessoas vivas) e imagens fraudulentas (como fotos de fotos, vídeos ou máscaras).\n",
    "- **Pré-processamento:** Normalizar e padronizar as imagens, redimensionando para um tamanho uniforme (por exemplo, 224x224 pixels) e aplicando data augmentation (como rotação, espelhamento, e ajustes de brilho) para aumentar a robustez do modelo.\n",
    "- **Divisão dos Dados:** Dividir o conjunto de dados em conjuntos de treinamento, validação e teste.\n",
    "\n",
    "#### 2. Detector de Faces\n",
    "- **Implementação:** Utilizar uma biblioteca como OpenCV, dlib ou MTCNN para detectar e extrair regiões de interesse (ROIs) contendo rostos nas imagens.\n",
    "- **Extração de Faces:** Aplicar o detector de faces para isolar as faces das imagens coletadas, garantindo que apenas as áreas relevantes sejam usadas no treinamento do modelo.\n",
    "\n",
    "#### 3. Extração de Características Faciais\n",
    "- **Landmarks Faciais:** Utilizar técnicas de landmarks faciais para mapear pontos-chave no rosto, como olhos, nariz e boca.\n",
    "- **Embeddings Faciais:** Gerar embeddings faciais usando redes neurais pré-treinadas como FaceNet ou VGGFace para representar características únicas de cada rosto.\n",
    "\n",
    "#### 4. Construção do Modelo de Detecção de Vivacidade\n",
    "- **Arquitetura da CNN:** Desenvolver uma rede neural convolucional (CNN) personalizada ou utilizar uma arquitetura pré-treinada (como ResNet, VGG ou MobileNet) para classificar as imagens.\n",
    "- **Treinamento do Modelo:** Treinar a CNN com o conjunto de dados preparado, utilizando técnicas de data augmentation para melhorar a generalização do modelo.\n",
    "- **Classificação de Vivacidade:** O modelo deverá classificar cada imagem com uma pontuação de probabilidade indicando a vivacidade (real ou fraudulenta).\n",
    "\n",
    "#### 5. Avaliação e Ajuste do Modelo\n",
    "- **Métricas de Avaliação:** Utilizar métricas como acurácia, precisão, recall e F1-score para avaliar o desempenho do modelo no conjunto de validação.\n",
    "- **Ajuste de Hiperparâmetros:** Realizar tuning de hiperparâmetros (como taxa de aprendizado, número de camadas e neurônios) para otimizar o desempenho do modelo.\n",
    "- **Fine-tuning:** Ajustar a rede neural pré-treinada (se utilizada) para adaptar melhor ao nosso conjunto de dados específico.\n",
    "\n",
    "#### 6. Implementação e Integração\n",
    "- **Sistema de Autenticação:** Integrar o modelo de detecção de vivacidade no sistema de autenticação da fintech.\n",
    "- **Fluxo de Verificação:** Implementar um fluxo de verificação onde o modelo analisa a imagem apresentada durante o processo de autenticação e retorna uma decisão de vivacidade.\n",
    "\n",
    "#### 7. Validação e Aprimoramento Contínuo\n",
    "- **Monitoramento:** Monitorar o desempenho do sistema de detecção de vivacidade em ambiente de produção.\n",
    "- **Feedback Loop:** Coletar feedback e dados adicionais para ajustar e melhorar continuamente o modelo.\n",
    "- **Atualização do Modelo:** Realizar atualizações periódicas do modelo com novos dados e técnicas para garantir que o sistema permaneça eficaz contra novas tentativas de fraude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Demonstração\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Carregar e pré-processar os dados\n",
    "def load_and_preprocess_data(data_dir):\n",
    "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='binary',\n",
    "        subset='training'\n",
    "    )\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='binary',\n",
    "        subset='validation'\n",
    "    )\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Construção do modelo CNN\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Diretório de dados\n",
    "data_dir = 'path_to_your_dataset'\n",
    "\n",
    "# Carregar os dados\n",
    "train_generator, validation_generator = load_and_preprocess_data(data_dir)\n",
    "\n",
    "# Construir o modelo\n",
    "model = build_model()\n",
    "\n",
    "# Treinar o modelo\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "\n",
    "# Avaliar o modelo\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f'Acurácia: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Salvar o modelo treinado\n",
    "model.save('liveness_detection_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Desenvolvimento da solução (5,5 pontos)\n",
    "\n",
    "Detalhe o passo-a-passo do algoritmo de deteção de vivacidade.\n",
    "Se optar pela construção e treinamento de um modelo de redes neurais convulucionais, apresente a arquitetura, prepare os dados de treinamento, realize o treinamento.\n",
    "\n",
    "**resposta**\n",
    "\n",
    "Passo 1: Coleta e Preparação dos Dados\n",
    "\n",
    "Coleta de Dados: Reunir imagens contendo rostos reais e imagens fraudulentas (fotos de fotos, vídeos ou máscaras).\n",
    "Pré-processamento: Redimensionar todas as imagens para 224x224 pixels e normalizá-las.\n",
    "Data Augmentation: Aplicar técnicas de aumento de dados como rotação, espelhamento e ajustes de brilho para aumentar a robustez do modelo.\n",
    "\n",
    "Divisão dos Dados: Dividir os dados em conjuntos de treinamento, validação e teste (80% treinamento, 10% validação, 10% teste).\n",
    "\n",
    "Passo 2: Implementação do Detector de Faces\n",
    "\n",
    "Utilização de Bibliotecas: Utilizar bibliotecas como OpenCV, dlib ou MTCNN para detectar faces nas imagens.\n",
    "Extração de ROIs: Isolar e extrair as regiões contendo rostos nas imagens.\n",
    "\n",
    "Passo 3: Extração de Características Faciais\n",
    "\n",
    "Landmarks Faciais: Mapear pontos-chave no rosto (olhos, nariz, boca) utilizando técnicas de landmarks faciais.\n",
    "Embeddings Faciais: Gerar embeddings faciais utilizando redes neurais pré-treinadas como FaceNet ou VGGFace.\n",
    "\n",
    "Passo 4: Construção e Treinamento do Modelo de Redes Neurais Convolucionais (CNN)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arquitetura da CNN:\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparação dos Dados de Treinamento:\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def load_and_preprocess_data(data_dir):\n",
    "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='binary',\n",
    "        subset='training'\n",
    "    )\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='binary',\n",
    "        subset='validation'\n",
    "    )\n",
    "    return train_generator, validation_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do Modelo:\n",
    "\n",
    "# Diretório de dados\n",
    "data_dir = ''\n",
    "\n",
    "# Carregar os dados\n",
    "train_generator, validation_generator = load_and_preprocess_data(data_dir)\n",
    "\n",
    "# Construir o modelo\n",
    "model = build_model()\n",
    "\n",
    "# Treinar o modelo\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação do Modelo:\n",
    "\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f'Acurácia: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o modelo treinado\n",
    "model.save('liveness_detection_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Organização de dados para treinamento de modelo de liveness (2 pontos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPLEMENTAR\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def resize_images(input_dir, output_dir, size=(224, 224)):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for filename in os.listdir(input_dir):\n",
    "        img = cv2.imread(os.path.join(input_dir, filename))\n",
    "        if img is not None:\n",
    "            resized_img = cv2.resize(img, size)\n",
    "            cv2.imwrite(os.path.join(output_dir, filename), resized_img)\n",
    "\n",
    "resize_images('path_to_raw_images/liveness', 'path_to_preprocessed_images/liveness')\n",
    "resize_images('path_to_raw_images/fraudulent', 'path_to_preprocessed_images/fraudulent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalização das Imagens: Normalizar os valores dos pixels para o intervalo [0, 1].\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation: Aplicar técnicas de data augmentation para aumentar a diversidade do conjunto de treinamento, como rotação, espelhamento, zoom e ajuste de brilho.\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Estrutura de Diretórios: Organizar os dados em diretórios separados para treinamento, validação e teste.\n",
    "# path_to_preprocessed_images/\n",
    "#     liveness/\n",
    "#         img1.jpg\n",
    "#         img2.jpg\n",
    "#         ...\n",
    "#     fraudulent/\n",
    "#         img1.jpg\n",
    "#         img2.jpg\n",
    "#         ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento, Validação e Teste: Dividir os dados em conjuntos de treinamento (80%), validação (10%) e teste (10%).\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "liveness_images = os.listdir('path_to_preprocessed_images/liveness')\n",
    "fraudulent_images = os.listdir('path_to_preprocessed_images/fraudulent')\n",
    "\n",
    "# Criar listas de caminhos completos das imagens\n",
    "liveness_images = ['path_to_preprocessed_images/liveness/' + img for img in liveness_images]\n",
    "fraudulent_images = ['path_to_preprocessed_images/fraudulent/' + img for img in fraudulent_images]\n",
    "\n",
    "# Combinar as listas e criar etiquetas\n",
    "all_images = liveness_images + fraudulent_images\n",
    "labels = [0] * len(liveness_images) + [1] * len(fraudulent_images)\n",
    "\n",
    "# Dividir os dados\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(all_images, labels, test_size=0.2, random_state=42)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Utilização do ImageDataGenerator: Usar o ImageDataGenerator para carregar e preparar os dados durante o treinamento.\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'path_to_train_data',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    'path_to_validation_data',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'path_to_test_data',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Treinamento de modelo de liveness (1,5 pontos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPLEMENTAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Métricas de desempenho do modelo (2 pontos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPLEMENTAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Teste Fim-a-Fim\n",
    "\n",
    "Simule a operação fim-a-fim, com uma imagem de entrada forjada (foto de foto de um rosto) e outra com uma imagem de rosto, exibindo o resultado da classificação e a pontuação de cada classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPLEMENTAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Com a implementação da solução na forma de uma aplicação do [Streamlit](https://www.streamlit.io/) (veja a pata streamlit-app e use o template) vale 1 ponto adicional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pergunta**: Se utilizou o Streamlit, compartilhe a URL do aplicativo publicado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resposta**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Conclusões (2,5 pontos)\n",
    "\n",
    "**Pergunta**: Dado todo o estudo e pesquisa, quais foram as conclusões sobre a solução, o que funcionou, o que não funcionou e quais os detalhes que observariam numa nova versão e melhorias do processo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resposta**:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "733a071da2455ea0e8bdf5409a7097e630ac701195faf55c6e985d77ee3ec176"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
